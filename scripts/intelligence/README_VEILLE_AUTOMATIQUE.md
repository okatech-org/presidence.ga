# ü¶Ö Syst√®me de Veille Automatique - Lynx Eye

## Vue d'ensemble

Le syst√®me de veille automatique collecte automatiquement des informations depuis les sources configur√©es (RSS, r√©seaux sociaux, etc.) et les analyse avec l'IA pour g√©n√©rer des insights strat√©giques.

## ‚ú® Fonctionnalit√©s

- ‚öôÔ∏è **Configuration personnalisable** : Fr√©quence, r√©seaux sociaux actifs
- üîÑ **Collecte automatique** : Scraping p√©riodique selon la configuration
- üß† **Analyse IA automatique** : Chaque item collect√© est analys√© (r√©sum√©, cat√©gorie, sentiment, entit√©s)
- üìä **Interface de gestion** : Dashboard complet dans l'espace Admin
- üéØ **Suivi en temps r√©el** : Derni√®re et prochaine collecte visibles

## üìã Sources surveill√©es

Le syst√®me surveille actuellement :
- **RSS** : Sources d'actualit√©s gabonaises
- **Facebook** : Recherches sur "Gabon"
- **TikTok** : Contenus li√©s au Gabon
- **YouTube** : Vid√©os sur le Gabon
- **X (Twitter)** : Mentions du Gabon

## üéõÔ∏è Configuration

### Via l'interface Admin

1. Allez dans **Admin Space** ‚Üí **Lynx Eye** ‚Üí **Configuration**
2. Configurez :
   - **Syst√®me actif** : Activer/d√©sactiver la collecte
   - **Fr√©quence** : Intervalle en heures (par d√©faut: 72h = 3 jours)
   - **R√©seaux sociaux** : Activer/d√©sactiver chaque r√©seau
3. Cliquez sur **Enregistrer**
4. Utilisez **Lancer maintenant** pour une collecte imm√©diate

### Configuration par d√©faut

```json
{
  "enabled": true,
  "frequency_hours": 72,
  "social_networks": {
    "facebook": true,
    "tiktok": true,
    "youtube": true,
    "x": true
  }
}
```

## üöÄ Mise en place du Cron Job

Pour automatiser compl√®tement la collecte, vous devez configurer un cron job Supabase :

### Option 1 : Via SQL Editor

1. Ouvrez votre projet Supabase
2. Allez dans **SQL Editor**
3. Ouvrez le fichier `scripts/intelligence/setup_cron_supabase.sql`
4. **Remplacez les variables** :
   - `YOUR_PROJECT_URL` ‚Üí URL de votre projet (ex: https://xxxxx.supabase.co)
   - `YOUR_ANON_KEY` ‚Üí Votre cl√© anon (Settings > API)
5. Ex√©cutez le script

### Option 2 : Configuration manuelle

```sql
SELECT cron.schedule(
    'intelligence-scraping-check',
    '0 * * * *',  -- Toutes les heures
    $$
    SELECT
      net.http_post(
          url:='https://votre-projet.supabase.co/functions/v1/trigger-intelligence-scraping',
          headers:='{"Content-Type": "application/json", "Authorization": "Bearer votre-cle-anon"}'::jsonb,
          body:='{}'::jsonb
      ) as request_id;
    $$
);
```

### V√©rification du cron

```sql
-- Voir le job
SELECT * FROM cron.job WHERE jobname = 'intelligence-scraping-check';

-- Voir l'historique d'ex√©cution
SELECT * FROM cron.job_run_details 
WHERE jobid = (SELECT jobid FROM cron.job WHERE jobname = 'intelligence-scraping-check')
ORDER BY start_time DESC LIMIT 10;
```

## üîÑ Fonctionnement

### 1. D√©clenchement
- **Automatique** : Le cron job v√©rifie toutes les heures
- **Manuel** : Bouton "Lancer maintenant" dans l'interface

### 2. Processus de collecte

```mermaid
graph TD
    A[D√©clenchement] --> B{Syst√®me activ√©?}
    B -->|Non| C[Arr√™t]
    B -->|Oui| D[R√©cup√©rer sources actives]
    D --> E[Pour chaque source]
    E --> F[Collecter les donn√©es]
    F --> G[Ins√©rer dans intelligence_items]
    G --> H[Trigger analyse IA automatique]
    H --> I[Analyse Gemini + Embedding OpenAI]
    I --> J[Mise √† jour avec r√©sultats]
    J --> K[Source suivante]
    K --> E
    E --> L[Mettre √† jour last_run_at]
    L --> M[Calculer next_run_at]
    M --> N[Fin]
```

### 3. Analyse automatique

Chaque item collect√© d√©clenche automatiquement :
- **Analyse Gemini** : R√©sum√©, cat√©gorie, sentiment, entit√©s
- **Embedding OpenAI** : Vectorisation pour recherche s√©mantique
- **Logs** : Suivi dans `intelligence_processing_logs`

## üìä Surveillance

### Dashboard Intelligence

- **Donn√©es Captur√©es** : Nombre total d'items vectoris√©s
- **Sources Actives** : Canaux d'√©coute op√©rationnels
- **Alertes 24h** : Signaux faibles ou critiques

### Logs de traitement

```sql
-- Voir les derniers traitements
SELECT * FROM intelligence_processing_logs
ORDER BY started_at DESC
LIMIT 20;

-- Voir les erreurs
SELECT * FROM intelligence_processing_logs
WHERE status = 'error'
ORDER BY started_at DESC;
```

## üîß Personnalisation

### Ajouter une source

```sql
INSERT INTO intelligence_sources (name, type, url, status)
VALUES ('Nouvelle Source', 'rss', 'https://example.com/feed', 'active');
```

### Modifier la fr√©quence

Via l'interface Admin ou directement :

```sql
UPDATE intelligence_scraping_config
SET frequency_hours = 48  -- 2 jours
WHERE id = '00000000-0000-0000-0000-000000000001';
```

## üö® D√©pannage

### Le cron ne s'ex√©cute pas

1. V√©rifier que `pg_cron` est activ√© :
```sql
SELECT * FROM pg_extension WHERE extname = 'pg_cron';
```

2. V√©rifier les credentials dans le cron job

### Pas de donn√©es collect√©es

1. V√©rifier que le syst√®me est activ√©
2. V√©rifier les logs :
```sql
SELECT * FROM intelligence_processing_logs
WHERE status = 'error';
```

3. Tester manuellement :
   - Cliquez sur "Lancer maintenant" dans l'interface

### Erreurs d'analyse IA

- V√©rifier que `GEMINI_API_KEY` et `OPENAI_API_KEY` sont configur√©s
- Consulter les logs de l'Edge Function `process-intelligence`

## üìù Notes importantes

- **Performance** : Le scraping de r√©seaux sociaux est simul√© actuellement. Pour une vraie collecte, vous devrez :
  - Utiliser les APIs officielles (Facebook Graph API, Twitter API, etc.)
  - Impl√©menter des scrapers externes avec des biblioth√®ques comme Puppeteer
  - Respecter les CGU et limites de rate des plateformes

- **Co√ªts** : Chaque analyse consomme des cr√©dits API (Gemini + OpenAI)

- **S√©curit√©** : Les cl√©s API sont stock√©es de mani√®re s√©curis√©e dans les Secrets Supabase

## üîó Ressources

- [Documentation Supabase Cron](https://supabase.com/docs/guides/database/extensions/pg_cron)
- [Guide de d√©ploiement complet](./README_DEPLOYMENT.md)
- [Configuration des sources](./sources.py)
