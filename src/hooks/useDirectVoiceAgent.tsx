import { useState, useRef, useCallback } from 'react';
import { supabase } from '@/integrations/supabase/client';
import { useToast } from '@/hooks/use-toast';

interface UseDirectVoiceAgentProps {
  userRole: 'president' | 'minister' | 'default';
  onSpeakingChange?: (isSpeaking: boolean) => void;
}

export const useDirectVoiceAgent = ({ 
  userRole,
  onSpeakingChange 
}: UseDirectVoiceAgentProps) => {
  const { toast } = useToast();
  const [isRecording, setIsRecording] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [conversationActive, setConversationActive] = useState(false);
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const audioContextRef = useRef<AudioContext | null>(null);
  const currentAudioRef = useRef<HTMLAudioElement | null>(null);
  const conversationHistoryRef = useRef<Array<{ role: string; content: string }>>([]);

  const startRecording = useCallback(async () => {
    try {
      console.log('üé§ Starting recording...');
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      
      const mediaRecorder = new MediaRecorder(stream);
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        console.log('‚èπÔ∏è Recording stopped, processing...');
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        await processAudio(audioBlob);
        
        // Stop all tracks
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorder.start();
      setIsRecording(true);
      console.log('‚úÖ Recording started');
    } catch (error) {
      console.error('‚ùå Error starting recording:', error);
      toast({
        title: "Erreur microphone",
        description: "Impossible d'acc√©der au microphone",
        variant: "destructive",
      });
    }
  }, [toast]);

  const stopRecording = useCallback(() => {
    if (mediaRecorderRef.current && isRecording) {
      console.log('üõë Stopping recording...');
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  }, [isRecording]);

  const processAudio = async (audioBlob: Blob) => {
    setIsProcessing(true);
    
    try {
      // 1. Transcription avec speech-to-text
      console.log('üìù Transcribing audio...');
      const formData = new FormData();
      formData.append('audio', audioBlob);
      
      const { data: transcription, error: transcriptionError } = await supabase.functions.invoke(
        'speech-to-text',
        {
          body: formData,
        }
      );

      if (transcriptionError) throw transcriptionError;
      if (!transcription?.text) throw new Error('No transcription received');

      const userText = transcription.text;
      console.log('‚úÖ Transcription:', userText);

      // Ajouter √† l'historique
      conversationHistoryRef.current.push({
        role: 'user',
        content: userText,
      });

      // 2. G√©n√©rer la r√©ponse avec chat-iasted
      console.log('ü§ñ Generating response...');
      const { data: chatResponse, error: chatError } = await supabase.functions.invoke(
        'chat-iasted',
        {
          body: {
            messages: conversationHistoryRef.current,
            userRole,
          },
        }
      );

      if (chatError) throw chatError;
      if (!chatResponse?.response) throw new Error('No response received');

      const assistantText = chatResponse.response;
      console.log('‚úÖ Response:', assistantText);

      // Ajouter √† l'historique
      conversationHistoryRef.current.push({
        role: 'assistant',
        content: assistantText,
      });

      // 3. Synth√®se vocale avec text-to-speech
      console.log('üîä Generating speech...');
      const { data: audioData, error: ttsError } = await supabase.functions.invoke(
        'text-to-speech',
        {
          body: {
            text: assistantText,
            userRole,
          },
        }
      );

      if (ttsError) throw ttsError;
      if (!audioData) throw new Error('No audio data received');

      // 4. Jouer l'audio
      await playAudio(audioData);

    } catch (error) {
      console.error('‚ùå Error processing audio:', error);
      toast({
        title: "Erreur de traitement",
        description: error instanceof Error ? error.message : "Une erreur est survenue",
        variant: "destructive",
      });
    } finally {
      setIsProcessing(false);
    }
  };

  const playAudio = async (audioData: ArrayBuffer | Blob) => {
    try {
      console.log('üéµ Playing audio response...');
      setIsSpeaking(true);
      onSpeakingChange?.(true);

      // Cr√©er un blob si n√©cessaire
      const blob = audioData instanceof Blob 
        ? audioData 
        : new Blob([audioData], { type: 'audio/mpeg' });
      
      const audioUrl = URL.createObjectURL(blob);
      const audio = new Audio(audioUrl);
      currentAudioRef.current = audio;

      audio.onended = () => {
        console.log('‚úÖ Audio playback finished');
        setIsSpeaking(false);
        onSpeakingChange?.(false);
        URL.revokeObjectURL(audioUrl);
        
        // Si la conversation est active, recommencer l'enregistrement
        if (conversationActive) {
          setTimeout(() => {
            startRecording();
          }, 500);
        }
      };

      audio.onerror = (error) => {
        console.error('‚ùå Audio playback error:', error);
        setIsSpeaking(false);
        onSpeakingChange?.(false);
        URL.revokeObjectURL(audioUrl);
      };

      await audio.play();
    } catch (error) {
      console.error('‚ùå Error playing audio:', error);
      setIsSpeaking(false);
      onSpeakingChange?.(false);
    }
  };

  const startConversation = useCallback(async () => {
    console.log('üöÄ Starting conversation...');
    
    try {
      // Demander l'acc√®s au microphone
      await navigator.mediaDevices.getUserMedia({ audio: true });
      
      setConversationActive(true);
      conversationHistoryRef.current = [];
      
      toast({
        title: "Conversation d√©marr√©e",
        description: "Parlez maintenant, je vous √©coute",
      });

      // D√©marrer l'enregistrement
      await startRecording();
      
    } catch (error) {
      console.error('‚ùå Error starting conversation:', error);
      toast({
        title: "Erreur",
        description: "Impossible de d√©marrer la conversation",
        variant: "destructive",
      });
    }
  }, [startRecording, toast]);

  const stopConversation = useCallback(() => {
    console.log('‚èπÔ∏è Stopping conversation...');
    
    setConversationActive(false);
    
    // Arr√™ter l'enregistrement si actif
    if (isRecording) {
      stopRecording();
    }
    
    // Arr√™ter l'audio si en cours
    if (currentAudioRef.current) {
      currentAudioRef.current.pause();
      currentAudioRef.current = null;
    }
    
    // Nettoyer l'historique
    conversationHistoryRef.current = [];
    
    setIsSpeaking(false);
    setIsProcessing(false);
    onSpeakingChange?.(false);
    
    toast({
      title: "Conversation termin√©e",
      description: "iAsted est en veille",
    });
  }, [isRecording, stopRecording, onSpeakingChange, toast]);

  return {
    // √âtats
    isConnected: conversationActive,
    isSpeaking,
    isRecording,
    isProcessing,
    
    // Actions
    startConversation,
    stopConversation,
    startRecording,
    stopRecording,
  };
};
