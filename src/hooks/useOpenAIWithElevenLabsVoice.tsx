/**
 * Hook hybride: OpenAI GPT pour l'intelligence + ElevenLabs pour la voix
 * Combine le meilleur des deux mondes
 */

import { useState, useRef, useCallback, useEffect } from 'react';
import { supabase } from '@/integrations/supabase/client';
import { useToast } from '@/hooks/use-toast';

type VoiceState = 'idle' | 'connecting' | 'listening' | 'thinking' | 'speaking';

interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: string;
}

interface UseOpenAIWithElevenLabsVoiceOptions {
  voiceId?: string;
  systemPrompt?: string;
  onMessage?: (message: Message) => void;
}

class AudioRecorder {
  private stream: MediaStream | null = null;
  private audioContext: AudioContext | null = null;
  private processor: ScriptProcessorNode | null = null;
  private source: MediaStreamAudioSourceNode | null = null;
  private isRecording = false;

  constructor(private onAudioData: (audioData: Float32Array) => void) {}

  async start() {
    if (this.isRecording) return;
    
    try {
      this.stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 24000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });
      
      this.audioContext = new AudioContext({ sampleRate: 24000 });
      this.source = this.audioContext.createMediaStreamSource(this.stream);
      this.processor = this.audioContext.createScriptProcessor(4096, 1, 1);
      
      this.processor.onaudioprocess = (e) => {
        if (this.isRecording) {
          const inputData = e.inputBuffer.getChannelData(0);
          this.onAudioData(new Float32Array(inputData));
        }
      };
      
      this.source.connect(this.processor);
      this.processor.connect(this.audioContext.destination);
      this.isRecording = true;
      
      console.log('‚úÖ [AudioRecorder] Enregistrement d√©marr√©');
    } catch (error) {
      console.error('‚ùå [AudioRecorder] Erreur acc√®s microphone:', error);
      throw error;
    }
  }

  pause() {
    this.isRecording = false;
  }

  resume() {
    this.isRecording = true;
  }

  stop() {
    this.isRecording = false;
    if (this.source) {
      this.source.disconnect();
      this.source = null;
    }
    if (this.processor) {
      this.processor.disconnect();
      this.processor = null;
    }
    if (this.stream) {
      this.stream.getTracks().forEach(track => track.stop());
      this.stream = null;
    }
    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }
    console.log('‚úÖ [AudioRecorder] Enregistrement arr√™t√©');
  }
}

export const useOpenAIWithElevenLabsVoice = ({
  voiceId = 'EV6XgOdBELK29O2b4qyM', // Voix iAsted Pro par d√©faut
  systemPrompt = "Vous √™tes iAsted, l'assistant vocal intelligent du Pr√©sident de la R√©publique. Vous √™tes professionnel, concis et efficace. Vos r√©ponses sont claires et directes.",
  onMessage,
}: UseOpenAIWithElevenLabsVoiceOptions = {}) => {
  const [voiceState, setVoiceState] = useState<VoiceState>('idle');
  const [messages, setMessages] = useState<Message[]>([]);
  const [isConnected, setIsConnected] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  
  const recorderRef = useRef<AudioRecorder | null>(null);
  const audioQueueRef = useRef<string[]>([]);
  const currentAudioRef = useRef<HTMLAudioElement | null>(null);
  const audioChunksRef = useRef<Float32Array[]>([]);
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const conversationHistoryRef = useRef<Array<{role: string, content: string}>>([]);
  
  const { toast } = useToast();

  // D√©tection de silence
  const detectSilence = useCallback((audioData: Float32Array) => {
    const rms = Math.sqrt(audioData.reduce((sum, val) => sum + val * val, 0) / audioData.length);
    return rms < 0.01; // Seuil de silence
  }, []);

  // Encoder l'audio pour l'envoyer √† Whisper
  const encodeAudioForWhisper = useCallback((chunks: Float32Array[]): string => {
    const totalLength = chunks.reduce((sum, chunk) => sum + chunk.length, 0);
    const combined = new Float32Array(totalLength);
    let offset = 0;
    for (const chunk of chunks) {
      combined.set(chunk, offset);
      offset += chunk.length;
    }

    const int16Array = new Int16Array(combined.length);
    for (let i = 0; i < combined.length; i++) {
      const s = Math.max(-1, Math.min(1, combined[i]));
      int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    
    const uint8Array = new Uint8Array(int16Array.buffer);
    let binary = '';
    const chunkSize = 0x8000;
    
    for (let i = 0; i < uint8Array.length; i += chunkSize) {
      const chunk = uint8Array.subarray(i, Math.min(i + chunkSize, uint8Array.length));
      binary += String.fromCharCode(...Array.from(chunk));
    }
    
    return btoa(binary);
  }, []);

  // Transcrire l'audio avec Whisper
  const transcribeAudio = useCallback(async (audioBase64: string): Promise<string> => {
    try {
      console.log('üé§ [Hybrid] Transcription audio...');
      const { data, error } = await supabase.functions.invoke('transcribe-audio', {
        body: { audio: audioBase64 }
      });

      if (error) throw error;
      
      console.log('‚úÖ [Hybrid] Transcription:', data.text);
      return data.text;
    } catch (error) {
      console.error('‚ùå [Hybrid] Erreur transcription:', error);
      throw error;
    }
  }, []);

  // Obtenir une r√©ponse d'OpenAI
  const getOpenAIResponse = useCallback(async (userMessage: string): Promise<string> => {
    try {
      console.log('ü§ñ [Hybrid] Requ√™te OpenAI GPT...');
      
      conversationHistoryRef.current.push({
        role: 'user',
        content: userMessage
      });

      const { data, error } = await supabase.functions.invoke('chat-with-iasted', {
        body: {
          message: userMessage,
          conversationHistory: conversationHistoryRef.current,
          systemPrompt
        }
      });

      if (error) throw error;
      
      const assistantResponse = data.response;
      console.log('‚úÖ [Hybrid] R√©ponse OpenAI:', assistantResponse);
      
      conversationHistoryRef.current.push({
        role: 'assistant',
        content: assistantResponse
      });

      return assistantResponse;
    } catch (error) {
      console.error('‚ùå [Hybrid] Erreur OpenAI:', error);
      throw error;
    }
  }, [systemPrompt]);

  // Synth√©tiser avec ElevenLabs
  const synthesizeSpeech = useCallback(async (text: string) => {
    try {
      console.log('üéôÔ∏è [Hybrid] Synth√®se vocale ElevenLabs...');
      const { data, error } = await supabase.functions.invoke('text-to-speech', {
        body: { 
          text, 
          voiceId,
          userRole: 'president'
        }
      });

      if (error) throw error;
      
      return data.audioContent;
    } catch (error) {
      console.error('‚ùå [Hybrid] Erreur synth√®se vocale:', error);
      throw error;
    }
  }, [voiceId]);

  // Lire l'audio
  const playAudio = useCallback(async (audioBase64: string) => {
    return new Promise<void>((resolve, reject) => {
      try {
        const audio = new Audio(`data:audio/mpeg;base64,${audioBase64}`);
        currentAudioRef.current = audio;
        
        audio.onplay = () => {
          console.log('üîä [Hybrid] Lecture audio d√©marr√©e');
          setVoiceState('speaking');
          setIsSpeaking(true);
        };
        
        audio.onended = () => {
          console.log('‚úÖ [Hybrid] Lecture audio termin√©e');
          setVoiceState('listening');
          setIsSpeaking(false);
          currentAudioRef.current = null;
          resolve();
        };
        
        audio.onerror = (e) => {
          console.error('‚ùå [Hybrid] Erreur lecture audio:', e);
          setVoiceState('listening');
          setIsSpeaking(false);
          currentAudioRef.current = null;
          reject(e);
        };
        
        audio.play();
      } catch (error) {
        console.error('‚ùå [Hybrid] Erreur playAudio:', error);
        setVoiceState('listening');
        setIsSpeaking(false);
        reject(error);
      }
    });
  }, []);

  // Traiter l'audio collect√©
  const processCollectedAudio = useCallback(async () => {
    if (audioChunksRef.current.length === 0) return;

    try {
      setVoiceState('thinking');
      recorderRef.current?.pause();

      // Transcrire
      const audioBase64 = encodeAudioForWhisper(audioChunksRef.current);
      audioChunksRef.current = [];
      
      const transcription = await transcribeAudio(audioBase64);
      
      if (!transcription || transcription.trim().length === 0) {
        console.log('‚ö†Ô∏è [Hybrid] Transcription vide, retour en √©coute');
        setVoiceState('listening');
        recorderRef.current?.resume();
        return;
      }

      // Ajouter le message utilisateur
      const userMsg: Message = {
        id: crypto.randomUUID(),
        role: 'user',
        content: transcription,
        timestamp: new Date().toISOString()
      };
      setMessages(prev => [...prev, userMsg]);
      onMessage?.(userMsg);

      // Obtenir la r√©ponse OpenAI
      const responseText = await getOpenAIResponse(transcription);

      // Ajouter le message assistant
      const assistantMsg: Message = {
        id: crypto.randomUUID(),
        role: 'assistant',
        content: responseText,
        timestamp: new Date().toISOString()
      };
      setMessages(prev => [...prev, assistantMsg]);
      onMessage?.(assistantMsg);

      // Synth√©tiser et lire
      const audioBase64Voice = await synthesizeSpeech(responseText);
      await playAudio(audioBase64Voice);

      // Reprendre l'√©coute
      recorderRef.current?.resume();
    } catch (error) {
      console.error('‚ùå [Hybrid] Erreur traitement:', error);
      toast({
        title: 'Erreur',
        description: 'Erreur lors du traitement de votre message',
        variant: 'destructive',
      });
      setVoiceState('listening');
      recorderRef.current?.resume();
    }
  }, [encodeAudioForWhisper, transcribeAudio, getOpenAIResponse, synthesizeSpeech, playAudio, toast, onMessage]);

  // G√©rer les chunks audio
  const handleAudioData = useCallback((audioData: Float32Array) => {
    audioChunksRef.current.push(new Float32Array(audioData));

    const isSilent = detectSilence(audioData);

    if (isSilent) {
      if (!silenceTimerRef.current) {
        silenceTimerRef.current = setTimeout(() => {
          console.log('ü§ê [Hybrid] Silence d√©tect√©, traitement...');
          processCollectedAudio();
          silenceTimerRef.current = null;
        }, 1000); // 1 seconde de silence
      }
    } else {
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
        silenceTimerRef.current = null;
      }
    }
  }, [detectSilence, processCollectedAudio]);

  // Connexion
  const connect = useCallback(async () => {
    if (isConnected) return;

    try {
      console.log('üîå [Hybrid] Connexion...');
      setVoiceState('connecting');

      // Demander l'acc√®s au micro
      await navigator.mediaDevices.getUserMedia({ audio: true });

      // D√©marrer l'enregistrement
      const recorder = new AudioRecorder(handleAudioData);
      await recorder.start();
      recorderRef.current = recorder;

      setIsConnected(true);
      setVoiceState('listening');
      
      toast({
        title: 'Mode iAsted Pro activ√©',
        description: 'OpenAI GPT + Voix ElevenLabs',
      });

      console.log('‚úÖ [Hybrid] Connect√© et en √©coute');
    } catch (error) {
      console.error('‚ùå [Hybrid] Erreur connexion:', error);
      toast({
        title: 'Erreur de connexion',
        description: error instanceof Error ? error.message : 'Impossible de d√©marrer',
        variant: 'destructive',
      });
      setVoiceState('idle');
    }
  }, [isConnected, handleAudioData, toast]);

  // D√©connexion
  const disconnect = useCallback(() => {
    console.log('üîå [Hybrid] D√©connexion...');
    
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }

    if (currentAudioRef.current) {
      currentAudioRef.current.pause();
      currentAudioRef.current = null;
    }

    recorderRef.current?.stop();
    recorderRef.current = null;

    audioChunksRef.current = [];
    conversationHistoryRef.current = [];

    setIsConnected(false);
    setVoiceState('idle');
    setIsSpeaking(false);
    
    console.log('‚úÖ [Hybrid] D√©connect√©');
  }, []);

  // Toggle
  const toggleConversation = useCallback(async () => {
    if (isConnected) {
      disconnect();
    } else {
      await connect();
    }
  }, [isConnected, connect, disconnect]);

  // Cleanup
  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return {
    voiceState,
    messages,
    isConnected,
    isSpeaking,
    connect,
    disconnect,
    toggleConversation,
  };
};
