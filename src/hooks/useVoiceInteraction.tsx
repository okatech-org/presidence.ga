import { useState, useCallback, useRef, useEffect } from 'react';
import { supabase } from '@/integrations/supabase/client';
import { useToast } from '@/hooks/use-toast';


export type VoiceState = 'idle' | 'listening' | 'thinking' | 'speaking';

export interface Message {
  role: 'user' | 'assistant';
  text: string;
  timestamp?: Date;
}

interface UseVoiceInteractionOptions {
  onSpeakingChange?: (isSpeaking: boolean) => void;
  silenceDuration?: number;
  silenceThreshold?: number;
  continuousMode?: boolean;
  voiceId?: string;
}

export function useVoiceInteraction(options: UseVoiceInteractionOptions = {}) {
  const { toast } = useToast();
  const {
    onSpeakingChange,
    silenceDuration = 2000,
    silenceThreshold = 10,
    continuousMode = false,
    voiceId,
  } = options;

  // √âtats
  const [voiceState, setVoiceState] = useState<VoiceState>('idle');
  const [sessionId, setSessionId] = useState<string | null>(null);
  const [userId, setUserId] = useState<string | null>(null);
  const [audioLevel, setAudioLevel] = useState(0);
  const [isPaused, setIsPaused] = useState(false);
  // Utiliser le voiceId fourni ou la voix iAsted Pro par d√©faut
  const [selectedVoiceId, setSelectedVoiceId] = useState<string>(voiceId || 'EV6XgOdBELK29O2b4qyM');
  
  // Log pour debug
  useEffect(() => {
    console.log('üéôÔ∏è [useVoiceInteraction] VoiceId actuel:', selectedVoiceId);
  }, [selectedVoiceId]);
  const [silenceDetected, setSilenceDetected] = useState(false);
  const [silenceTimeRemaining, setSilenceTimeRemaining] = useState(0);
  const [liveTranscript, setLiveTranscript] = useState<string>('');
  const [continuousModePaused, setContinuousModePaused] = useState(false);
  const [conversationMessages, setConversationMessages] = useState<Array<{ role: 'user' | 'assistant'; text: string; timestamp: Date }>>([]);

  // Refs pour l'enregistrement audio
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const currentAudioRef = useRef<HTMLAudioElement | null>(null);

  // Charger l'utilisateur
  useEffect(() => {
    const loadUser = async () => {
      // Essayer d'abord avec getSession qui est plus fiable
      const { data: { session } } = await supabase.auth.getSession();
      if (session?.user) {
        setUserId(session.user.id);
        console.log('[useVoiceInteraction] ‚úÖ Utilisateur charg√© via getSession:', session.user.id);
      } else {
        // Fallback sur getUser
        const { data: { user } } = await supabase.auth.getUser();
        if (user) {
          setUserId(user.id);
          console.log('[useVoiceInteraction] ‚úÖ Utilisateur charg√© via getUser:', user.id);
        } else {
          console.warn('[useVoiceInteraction] ‚ö†Ô∏è Aucun utilisateur trouv√©');
        }
      }
    };
    loadUser();
  }, []);

  // Mettre √† jour selectedVoiceId quand voiceId change
  useEffect(() => {
    if (voiceId) {
      console.log('[useVoiceInteraction] üéôÔ∏è Mise √† jour voice ID:', voiceId);
      setSelectedVoiceId(voiceId);
    }
  }, [voiceId]);

  // Cr√©er une nouvelle session
  const createSession = useCallback(async (): Promise<string> => {
    // V√©rifier d'abord si userId est d√©j√† charg√©
    let currentUserId = userId;
    
    // Si userId n'est pas encore charg√©, essayer de le r√©cup√©rer
    if (!currentUserId) {
      // Essayer d'abord avec getSession qui est plus fiable
      const { data: { session }, error: sessionError } = await supabase.auth.getSession();
      if (sessionError || !session?.user) {
        // Si getSession √©choue, essayer getUser
        const { data: { user }, error: authError } = await supabase.auth.getUser();
        if (authError || !user) {
          console.error('[createSession] Erreur authentification:', { sessionError, authError });
          throw new Error('User not authenticated. Please log in first.');
        }
        currentUserId = user.id;
        setUserId(currentUserId);
      } else {
        currentUserId = session.user.id;
        setUserId(currentUserId);
      }
    }

    const { data, error } = await supabase
      .from('conversation_sessions')
      .insert({
        user_id: currentUserId,
        started_at: new Date().toISOString(),
        settings: {
          voiceId: selectedVoiceId,
          silenceDuration,
          silenceThreshold,
          continuousMode,
        },
      })
      .select()
      .single();

    if (error) {
      console.error('[createSession] Erreur cr√©ation session:', error);
      throw error;
    }
    return data.id;
  }, [userId, selectedVoiceId, silenceDuration, silenceThreshold, continuousMode]);

  // D√©tecter le silence et mettre √† jour les √©tats
  const analyzeAudioLevel = useCallback(() => {
    if (!analyserRef.current) return;

    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
    analyserRef.current.getByteFrequencyData(dataArray);
    
    const average = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
    const normalizedLevel = Math.min(100, (average / 255) * 100);
    setAudioLevel(normalizedLevel);

    // D√©tection de silence automatique
    if (normalizedLevel < silenceThreshold) {
      if (!silenceTimerRef.current) {
        console.log('üîá D√©but de silence d√©tect√©');
        setSilenceDetected(true);
        let timeRemaining = silenceDuration;
        
        silenceTimerRef.current = setInterval(() => {
          timeRemaining -= 100;
          setSilenceTimeRemaining(timeRemaining);
          
          // Quand le silence atteint la dur√©e configur√©e, arr√™ter l'√©coute automatiquement
          if (timeRemaining <= 0 && voiceState === 'listening') {
            console.log('üîá Silence confirm√© - arr√™t automatique de l\'√©coute');
            clearInterval(silenceTimerRef.current!);
            silenceTimerRef.current = null;
            setSilenceDetected(false);
            setSilenceTimeRemaining(0);
            // Arr√™ter l'enregistrement - ceci va d√©clencher processAudio via mediaRecorder.onstop
            if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
              mediaRecorderRef.current.stop();
              setVoiceState('thinking');
            }
          }
        }, 100);
      }
    } else {
      // L'utilisateur parle √† nouveau, r√©initialiser le timer
      if (silenceTimerRef.current) {
        clearInterval(silenceTimerRef.current);
        silenceTimerRef.current = null;
        setSilenceDetected(false);
        setSilenceTimeRemaining(0);
      }
    }

    if (voiceState === 'listening') {
      requestAnimationFrame(analyzeAudioLevel);
    }
  }, [voiceState, silenceThreshold, silenceDuration]); // stopListening n'est pas inclus car on utilise directement mediaRecorderRef

  // D√©marrer l'√©coute
  const startListening = useCallback(async () => {
    try {
      console.log('üé§ D√©marrage de l\'√©coute...');

      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 24000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        } 
      });

      // Cr√©er le contexte audio pour l'analyse
      audioContextRef.current = new AudioContext({ sampleRate: 24000 });
      analyserRef.current = audioContextRef.current.createAnalyser();
      const source = audioContextRef.current.createMediaStreamSource(stream);
      source.connect(analyserRef.current);

      // Cr√©er le MediaRecorder
      const mediaRecorder = new MediaRecorder(stream);
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        console.log('‚èπÔ∏è Enregistrement arr√™t√©');
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        await processAudio(audioBlob);
        
        // Nettoyer
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorder.start();
      setVoiceState('listening');
      
      // D√©marrer l'analyse audio
      analyzeAudioLevel();

    } catch (error) {
      console.error('‚ùå Erreur microphone:', error);
      toast({
        title: "Erreur microphone",
        description: "Impossible d'acc√©der au microphone",
        variant: "destructive",
      });
    }
  }, [analyzeAudioLevel, toast]);

  // Arr√™ter l'√©coute
  const stopListening = useCallback(() => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      console.log('üõë Arr√™t de l\'enregistrement...');
      mediaRecorderRef.current.stop();
      setVoiceState('thinking');
    }

    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
  }, []);

  // Traiter l'audio
  const processAudio = async (audioBlob: Blob) => {
    if (!sessionId) {
      console.error('‚ùå Pas de sessionId');
      return;
    }

    try {
      console.log('üìù Traitement de l\'audio...');
      setVoiceState('thinking');

      // Convertir en base64
      const reader = new FileReader();
      const audioBase64 = await new Promise<string>((resolve, reject) => {
        reader.onloadend = () => {
          const base64 = reader.result as string;
          resolve(base64.split(',')[1]);
        };
        reader.onerror = reject;
        reader.readAsDataURL(audioBlob);
      });

      // Calculer la dur√©e de l'audio pour estimer la complexit√©
      const audioDurationMs = audioBlob.size / 16; // Estimation approximative
      
      // Temps de r√©flexion adaptatif (2-5 secondes selon la longueur)
      // Audio court (< 2s) = 2s de r√©flexion
      // Audio moyen (2-5s) = 3s de r√©flexion  
      // Audio long (> 5s) = 4-5s de r√©flexion
      let thinkingTime = 2000; // minimum 2 secondes
      if (audioDurationMs > 2000) thinkingTime = 3000;
      if (audioDurationMs > 5000) thinkingTime = Math.min(5000, 4000 + (audioDurationMs - 5000) / 10);
      
      console.log(`ü§î Temps de r√©flexion: ${thinkingTime}ms (dur√©e audio estim√©e: ${audioDurationMs}ms)`);

      // Appeler chat-with-iasted
      const { data, error } = await supabase.functions.invoke('chat-with-iasted', {
        body: {
          sessionId,
          userId,
          audioBase64,
          langHint: 'fr',
          voiceId: selectedVoiceId,
          generateAudio: true,
          userRole: 'president', // Application d√©di√©e √† la pr√©sidence
        },
      });

      if (error) throw error;

      console.log('‚úÖ R√©ponse re√ßue:', data);

      // Simuler le temps de r√©flexion avant de r√©pondre
      await new Promise(resolve => setTimeout(resolve, thinkingTime));

      // Ajouter les messages √† l'historique de la conversation
      if (data.transcript) {
        setConversationMessages(prev => [...prev, { 
          role: 'user', 
          text: data.transcript, 
          timestamp: new Date() 
        }]);
      }
      
      if (data.answer) {
        setConversationMessages(prev => [...prev, { 
          role: 'assistant', 
          text: data.answer, 
          timestamp: new Date() 
        }]);
      }

      // V√©rifier le routage
      if (data.route?.category === 'voice_command') {
        console.log('üéôÔ∏è Commande vocale d√©tect√©e:', data.route.command);
        handleVoiceCommand(data.route.command, data.route.args);
        
        // Mode continu - relancer l'√©coute si non-pause
        if (continuousMode && !isPaused) {
          setTimeout(() => {
            startListening();
          }, 500);
        } else {
          setVoiceState('idle');
        }
        return;
      }

      // Si demande de r√©sum√©
      if (data.route?.category === 'ask_resume') {
        console.log('üìã Demande de r√©sum√© d√©tect√©e');
        toast({
          title: "R√©sum√© de session",
          description: "G√©n√©ration du r√©sum√© en cours...",
        });
        // TODO: Appeler debrief-session
        setVoiceState('idle');
        return;
      }

      // R√©ponses normales (query ou small_talk)
      console.log('üí¨ R√©ponse:', data.answer);

      // Jouer l'audio
      if (data.audioContent) {
        await playAudioResponse(data.audioContent);
      }

      // Mode continu - relancer l'√©coute apr√®s avoir parl√©
      console.log('üîÑ Mode continu activ√©, relance de l\'√©coute...');
      setTimeout(() => {
        startListening();
      }, 500);

    } catch (error) {
      console.error('‚ùå Erreur traitement:', error);
      toast({
        title: "Erreur de traitement",
        description: error instanceof Error ? error.message : "Une erreur est survenue",
        variant: "destructive",
      });
      setVoiceState('idle');
    }
  };

  // Jouer la r√©ponse audio
  const playAudioResponse = async (audioBase64: string) => {
    return new Promise<void>((resolve, reject) => {
      try {
        console.log('üîä [playAudioResponse] D√©marrage lecture audio');
        console.log('üìä [playAudioResponse] Longueur base64:', audioBase64.length);
        console.log('üîç [playAudioResponse] Premiers chars:', audioBase64.substring(0, 50));
        
        setVoiceState('speaking');
        onSpeakingChange?.(true);

        const audio = new Audio(`data:audio/mpeg;base64,${audioBase64}`);
        currentAudioRef.current = audio;

        audio.onloadeddata = () => {
          console.log('‚úÖ [playAudioResponse] Audio charg√©, dur√©e:', audio.duration);
        };

        audio.onended = () => {
          console.log('‚úÖ [playAudioResponse] Lecture termin√©e');
          onSpeakingChange?.(false);
          currentAudioRef.current = null;
          resolve();
        };

        audio.onerror = (error) => {
          console.error('‚ùå [playAudioResponse] Erreur audio:', error);
          console.error('‚ùå [playAudioResponse] Audio error code:', audio.error?.code);
          console.error('‚ùå [playAudioResponse] Audio error message:', audio.error?.message);
          onSpeakingChange?.(false);
          currentAudioRef.current = null;
          reject(error);
        };

        console.log('‚ñ∂Ô∏è [playAudioResponse] Appel audio.play()...');
        audio.play().then(() => {
          console.log('üéµ [playAudioResponse] Audio en lecture');
        }).catch(reject);
      } catch (error) {
        console.error('‚ùå [playAudioResponse] Exception:', error);
        onSpeakingChange?.(false);
        reject(error);
      }
    });
  };

  // G√©rer les commandes vocales
  const handleVoiceCommand = (command: string, args: any) => {
    console.log('üéôÔ∏è Commande vocale:', command, args);
    
    switch (command) {
      case 'stop_listening':
        console.log('‚èπÔ∏è Commande: Arr√™ter l\'√©coute');
        stopConversation();
        toast({
          title: "√âcoute arr√™t√©e",
          description: "Conversation termin√©e",
        });
        break;
        
      case 'pause':
        console.log('‚è∏Ô∏è Commande: Pause');
        setIsPaused(true);
        setVoiceState('idle');
        toast({
          title: "Pause activ√©e",
          description: "Dites 'continue' pour reprendre",
        });
        break;
        
      case 'resume':
        console.log('‚ñ∂Ô∏è Commande: Reprendre');
        setIsPaused(false);
        startListening();
        toast({
          title: "Reprise",
          description: "Je vous √©coute √† nouveau",
        });
        break;
        
      case 'new_question':
        console.log('üîÑ Commande: Nouvelle question');
        if (currentAudioRef.current) {
          currentAudioRef.current.pause();
        }
        startListening();
        toast({
          title: "Nouvelle question",
          description: "Je vous √©coute",
        });
        break;
        
      case 'show_history':
        console.log('üìú Commande: Afficher historique');
        toast({
          title: "Historique",
          description: "Cette fonctionnalit√© arrive bient√¥t",
        });
        break;
        
      case 'change_voice':
        console.log('üéµ Commande: Changer de voix');
        toast({
          title: "Changement de voix",
          description: "Utilisez les param√®tres pour changer de voix",
        });
        break;
        
      default:
        console.warn('‚ö†Ô∏è Commande non reconnue:', command);
    }
  };

  // D√©marrer la conversation
  const startConversation = useCallback(async () => {
    try {
      console.log('üöÄ [startConversation] D√©but...');
      console.log('üîß [startConversation] selectedVoiceId:', selectedVoiceId);
      console.log('üë§ [startConversation] userId actuel:', userId);
      console.log('üìä [startConversation] √âtat actuel voiceState:', voiceState);

      // V√©rifier l'authentification avant de cr√©er la session
      if (!userId) {
        console.log('‚è≥ [startConversation] userId non charg√©, r√©cup√©ration...');
        // Essayer d'abord avec getSession qui est plus fiable
        const { data: { session }, error: sessionError } = await supabase.auth.getSession();
        if (sessionError || !session?.user) {
          // Si getSession √©choue, essayer getUser
          const { data: { user }, error: authError } = await supabase.auth.getUser();
          if (authError || !user) {
            console.error('‚ùå [startConversation] Erreur authentification:', { sessionError, authError });
            throw new Error('Vous devez √™tre connect√© pour utiliser iAsted. Veuillez vous connecter.');
          }
          setUserId(user.id);
          console.log('‚úÖ [startConversation] Utilisateur charg√© via getUser:', user.id);
        } else {
          setUserId(session.user.id);
          console.log('‚úÖ [startConversation] Utilisateur charg√© via getSession:', session.user.id);
        }
      }

      // Cr√©er une session
      console.log('üìù [startConversation] Cr√©ation session...');
      const newSessionId = await createSession();
      console.log('‚úÖ [startConversation] Session cr√©√©e:', newSessionId);
      setSessionId(newSessionId);

      // Message de bienvenue contextuel
      const hour = new Date().getHours();
      const greeting = hour < 18 ? "Bonjour" : "Bonsoir";
      const welcomeMessage = `${greeting} Excellence, je suis iAsted, votre assistant vocal intelligent. Comment puis-je vous √™tre utile ?`;

      console.log('üéôÔ∏è [startConversation] Message de bienvenue:', welcomeMessage);
      console.log('üé§ [startConversation] Appel text-to-speech...');
      console.log('üéôÔ∏è [startConversation] VoiceId envoy√©:', selectedVoiceId);

      const { data, error } = await supabase.functions.invoke('text-to-speech', {
        body: {
          text: welcomeMessage,
          voiceId: selectedVoiceId,
        },
      });

      console.log('üìä [startConversation] R√©ponse text-to-speech:', { data, error });

      if (error) {
        console.error('‚ùå [startConversation] Erreur text-to-speech:', error);
        throw new Error(`Erreur text-to-speech: ${error.message || 'Inconnue'}`);
      }

      if (!data) {
        console.error('‚ùå [startConversation] Pas de data dans la r√©ponse');
        throw new Error('Pas de donn√©es dans la r√©ponse text-to-speech');
      }

      console.log('‚úÖ [startConversation] Audio g√©n√©r√©, data:', data);

      // Jouer le message de bienvenue
      if (data?.audioContent) {
        console.log('üîä [startConversation] Lecture audio, longueur:', data.audioContent.length);
        await playAudioResponse(data.audioContent);
        console.log('‚úÖ [startConversation] Audio jou√© avec succ√®s');
      } else {
        console.error('‚ùå [startConversation] Pas de audioContent:', data);
        throw new Error('Pas d\'audioContent dans la r√©ponse');
      }

      // D√©marrer l'√©coute apr√®s le message de bienvenue
      console.log('üëÇ [startConversation] D√©marrage √©coute dans 500ms...');
      setTimeout(() => {
        startListening();
      }, 500);

      toast({
        title: "Conversation d√©marr√©e",
        description: "iAsted est √† votre √©coute",
      });

    } catch (error) {
      console.error('‚ùå [startConversation] Erreur compl√®te:', error);
      console.error('‚ùå [startConversation] Stack:', error instanceof Error ? error.stack : 'N/A');
      toast({
        title: "Erreur",
        description: error instanceof Error ? error.message : "Impossible de d√©marrer la conversation",
        variant: "destructive",
      });
      setVoiceState('idle');
    }
  }, [createSession, selectedVoiceId, startListening, toast, userId]);

  // Arr√™ter la conversation
  const stopConversation = useCallback(async () => {
    console.log('‚èπÔ∏è Arr√™t de la conversation...');

    stopListening();

    if (currentAudioRef.current) {
      currentAudioRef.current.pause();
      currentAudioRef.current = null;
    }

    // Terminer la session
    if (sessionId) {
      await supabase
        .from('conversation_sessions')
        .update({ ended_at: new Date().toISOString() })
        .eq('id', sessionId);
    }

    setVoiceState('idle');
    setSessionId(null);
    onSpeakingChange?.(false);

    toast({
      title: "Conversation termin√©e",
      description: "iAsted est en veille",
    });
  }, [sessionId, stopListening, onSpeakingChange, toast]);

  // Fonction pour interrompre et d√©marrer une nouvelle interaction
  const handleInteraction = useCallback(async () => {
    console.log('üéØ [handleInteraction] √âtat actuel:', voiceState);
    
    try {
      if (voiceState === 'idle') {
        console.log('‚ñ∂Ô∏è [handleInteraction] D√©marrage conversation...');
        await startConversation();
      } else if (voiceState === 'listening') {
        console.log('‚è∏Ô∏è [handleInteraction] Arr√™t √©coute...');
        stopListening();
      } else if (voiceState === 'speaking' && currentAudioRef.current) {
        console.log('‚è≠Ô∏è [handleInteraction] Interruption + nouvelle √©coute...');
        currentAudioRef.current.pause();
        startListening();
      } else {
        console.log('‚èπÔ∏è [handleInteraction] Arr√™t conversation...');
        stopConversation();
      }
    } catch (error) {
      console.error('‚ùå [handleInteraction] Erreur:', error);
      toast({
        title: "Erreur",
        description: error instanceof Error ? error.message : "Une erreur est survenue",
        variant: "destructive",
      });
      setVoiceState('idle');
    }
  }, [voiceState, startConversation, stopConversation, stopListening, startListening, toast]);

  // Fonction pour annuler l'interaction en cours
  const cancelInteraction = useCallback(() => {
    console.log('‚ùå Annulation de l\'interaction');
    
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
    }
    
    if (currentAudioRef.current) {
      currentAudioRef.current.pause();
      currentAudioRef.current = null;
    }
    
    if (silenceTimerRef.current) {
      clearInterval(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    
    setSilenceDetected(false);
    setSilenceTimeRemaining(0);
    setVoiceState('idle');
    onSpeakingChange?.(false);
    
    toast({
      title: "Interaction annul√©e",
      description: "L'interaction vocale a √©t√© interrompue",
    });
  }, [toast, onSpeakingChange]);

  // Toggle pause en mode continu
  const toggleContinuousPause = useCallback(() => {
    setContinuousModePaused(prev => !prev);
    toast({
      title: continuousModePaused ? "Mode continu repris" : "Mode continu en pause",
      description: continuousModePaused ? "iAsted recommence √† √©couter" : "iAsted ne relancera pas automatiquement",
    });
  }, [continuousModePaused, toast]);

  return {
    // √âtats
    voiceState,
    sessionId,
    audioLevel,
    isPaused,
    silenceDetected,
    silenceTimeRemaining,
    silenceDuration,
    liveTranscript,
    continuousMode,
    continuousModePaused,
    conversationMessages,
    
    // Getters
    isIdle: voiceState === 'idle',
    isListening: voiceState === 'listening',
    isThinking: voiceState === 'thinking',
    isSpeaking: voiceState === 'speaking',
    isActive: voiceState !== 'idle',
    
    // Actions
    startConversation,
    stopConversation,
    startListening,
    stopListening,
    handleInteraction,
    cancelInteraction,
    toggleContinuousPause,
    setSelectedVoiceId,
    togglePause: () => setIsPaused(prev => !prev),
    clearMessages: () => setConversationMessages([]),
  };
}
